[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "소개",
    "section": "",
    "text": "저는 다양한 언어와 수학을 좋아하는 사무엘(본명: 김상윤(金尙潤))입니다. 이 블로그는 외국어 공부, 언어학, 수학, 프로그래밍 등 제가 공부한 내용을 정리하고 공유하는 공간입니다.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/250515_네덜란드어_감탄문/index.html",
    "href": "posts/250515_네덜란드어_감탄문/index.html",
    "title": "네덜란드어 감탄문",
    "section": "",
    "text": "reddit r/ChatGPT에 누가 GPT더러 지금까지의 대화를 바탕으로 자신을 그려 달라고 부탁했다는 글을 읽었다.\n그 글을 읽고 나도 한번 GPT한테 GPT가 상상하는 나를 그려 달라고 해 봐야겠다는 생각이 들었다.\n그리고 이게 그 그림이다:\n\n\n\nGPT가 상상한 내 모습\n\n\n이 그림을 보고 나도 저렇게 잘생겼으면 좋겠다는 생각이 들었다 ㅋㅋ\nHow good it must be if I looked like that!\n그리고 요즘 네덜란드어에 빠져 있어서 이걸 네덜란드어로 작문해 보았다.\n❌ Hoe goed het zou zijn als ik zo eruitzie.\n이 문장은 틀린 문장이다.\nGPT의 말에 따르면 이렇게 써야 한단다:\n✅ Wat zou het mooi zijn als ik er zo uitzag.\n“내가 저렇게 생겼다면 얼마나 좋을까.”\n✅ Het zou geweldig zijn als ik er zo uitzag.\n“내가 저렇게 생겼다면 정말 멋질 텐데.”\n✅ Kon ik er maar zo uitzien!\n“나도 저렇게 생겼으면 좋겠다!”\n여기서 중요한 점은\n1. 가정법이기 때문에 과거형을 써야 한다는 점(uitzag, kon과 같이).\n(영어에서 가정할 때 “if I looked”라고 하지 “if I look”이라고 하지 않는 것처럼.) 2. “er zo uitzag”라고 써야 한다는 점.\n(이놈의 er은 네덜란드어에서 가장 어려운 문법인 것 같다.\n언제 쓰는지도 헷갈릴 뿐더러 위치도 언제 붙였다 떼야 하는지 익숙지 않다.)\n근데 나는 Wat zou het…으로 시작하는 문장이 낯설게 느껴졌다. GPT한테 물어 보니까 다음과 같단다:\nWat zou het leuk zijn\n형식: Wat + zou + het + 형용사 + zijn (+ als/om …)\n예: Wat zou het leuk zijn om met jou op vakantie te gaan!\n(너랑 같이 휴가 간다면 얼마나 즐거울까!)\n이건 가정 + 감탄이 섞인 문장. 감탄문이라기보단 감탄을 담은 조건문.\nHoe leuk zou het zijn\n형식: 수사의문문(rhetorische vraag)의 형식이지만 감탄의 의미로 사용되는 경우가 많음.\n예: Hoe leuk zou het zijn als we daar werkten!\n(우리 거기서 일한다면 얼마나 재밌을까!)\n이건 의문문 구조를 빌려온 감탄문\n\n\n\n\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/250330_자연어와_컴퓨터_언어/index.html",
    "href": "posts/250330_자연어와_컴퓨터_언어/index.html",
    "title": "자연어와 컴퓨터 언어",
    "section": "",
    "text": "국비 코딩 학원에서 코딩을 하다가 문득 이런 생각이 들었다:\n자연어는 극도로 높은 수준의 컴퓨터 언어라고 할 수 있지 않을까?\n까딱하면 실행이 안 되는 컴퓨터 언어와 달리,\n자연어에서는 사소한 문법 실수가 있어도 소통하는 데 지장이 없다.\n근데 이거 완전 html, css 아닌가?\n프로젝트에서 프론트 엔드를 맡게 되다 보니 이런 생각을 할 수 있게 된 것 같다.\n컴퓨터 언어에서는 변수가 중요하다.\n그렇다면 자연어에서는 변수를 어떻게 처리할까?\n자연어도 깊게 파고 들어가면 컴퓨터 언어와 비슷한 원리로 동작할 것 같다는 느낌이 든다.\n단지 표면상으로 드러나는 게 컴퓨터 언어와 너무 달라서 이 둘의 연관성을 찾기 힘들다는 문제가 있다.\n수학적으로 언어를 분석하면 컴퓨터 언어와의 공통점이 보이지 않을까 싶다.\n\n\n\n\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/250330_Crossy_Road와_확률/index.html",
    "href": "posts/250330_Crossy_Road와_확률/index.html",
    "title": "Crossy Road와 확률",
    "section": "",
    "text": "Crossy Road 플레이 캡처 사진\n\n\nCrossy Road(길건너 친구들)를 플레이하면서 이런 생각이 들었다.\n내가 한 스텝 건널 때마다 죽을 확률이 있지 않을까?\n물론 내가 한 스텝 건널 때마다 죽을 확률이 도로 위, 강 위, 철도 위인지에 따라 다르겠지만,\n평균적으로 한 스텝 건널 때마다 죽을 확률을 생각할 수 있지 않을까?\n나는 평균 200점에서 300점 정도가 나오는데,\n나보다 점수가 월등히 높게 나오는, 500점, 600점을 쉽게 받는 사람들은 한 스텝 건널 때의 죽을 확률이 나보다 현저히 낮지 않을까?\n한 스텝 건널 때마다 죽을 확률이 점수에 어떻게 영향을 미치게 될지 궁금해졌다.\n내가 로스의 확률론을 끝내고 나면 이에 대한 답을 얻을 수 있지 않을까.\n\n\n\n\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/250514_모자_문제-포함-배제의_원리-/index.html",
    "href": "posts/250514_모자_문제-포함-배제의_원리-/index.html",
    "title": "모자 문제(포함-배제의 원리)",
    "section": "",
    "text": "A First Course in Probability, 9th edition by Sheldon Ross\nChapter 2, Example 5m, p.39\n예제 5m \\(N\\)명의 남성이 파티에서 각자 자신의 모자를 방 한가운데에 던진다고 하자.\n모자들은 먼저 무작위로 섞이고, 각각의 남성이 모자를 하나씩 고른다.\n이때 아무도 자기 모자를 고르지 않을 확률은 얼마일까?\n\n이 문제는 처음 보면 당황스럽다.\n이 문제를 어떻게 접근해야 하나 하는 생각이 든다.\n확률론을 공부하면서 느낀 건데, 확률론은 난이도가 롤러코스터다.\n앞부분은 쉽고, 뒷부분은 서서히 어려워지는 미적분과 달리,\n확률론은 앞부분부터 히말라야 산맥 등정이다.\n그렇다고 미적분보다 훨씬 어렵다고 하기는 뭐한 게,\n확률론 특유의 논리를 체화하면 비슷한 느낌으로 다가오는 면이 있기 때문이다.\n이 문제는 포함-배제의 원리라는 아이디어만 있으면 풀 수 있다.\n포함-배제의 원리는 벤 다이어그램을 떠올리면 쉽다.\n\n\n\n벤 다이어그램\n\n\n우리가 구해야 하는 건 \\((A \\cup B \\cup C)^c\\)에 해당하는 부분이다.\n이를 어떻게 풀어 쓸 수 있을까?\n먼저 \\(A + B + C\\)를 하고 나면 겹치는 부분이 생긴다.\n겹치는 부분인 \\(A \\cap B\\), \\(B \\cap C\\), \\(C \\cap A\\)를 한 번씩 빼 줘야 한다.\n하지만 이렇게 하면 세 집합이 모두 겹치는 \\(A \\cap B \\cap C\\)가 세 번 빠지므로, 마지막에 한 번 더 더해 주어야 한다.\n이게 바로 포함-배제의 원리다!\n\\[\nP((A \\cup B \\cup C)^c) = 1 - P(A) - P(B) - P(C) + P(A \\cap B) + P(B \\cap C) + P(C \\cap A) - P(A \\cap B \\cap C)\n\\]\n그러면 이것을 확률론에 적용시키면 어떻게 될까?\n각 \\(i = 1, 2, \\dots, N\\)에 대해:\n\n\\(E_i\\): i번째 사람이 자기 모자를 고르는 사건\n이라 하면,\n우리가 원하는 사건은 \\(\\left( \\bigcup_{i=1}^N E_i \\right)^c\\)이다.\n\n포함-배제의 원리에 따라 다음과 같이 쓸 수 있다:\n\\[\nP\\left(\\bigcup_{i=1}^N E_i\\right)\n= \\sum_{i=1}^N P(E_i)\n- \\sum_{i_1 &lt; i_2} P(E_{i_1} E_{i_2})\n+ \\cdots\n+ (-1)^{n+1} \\sum_{i_1 &lt; \\cdots &lt; i_n} P(E_{i_1} \\cdots E_{i_n})\n+ \\cdots\n+ (-1)^{N+1} P(E_1 E_2 \\cdots E_N)\n\\] (이런 수식에 겁먹을 필요 없다. 이는 단지 포함-배제의 원리를 일반화한 수식에 불과하다.)\n각 항의 계산은 다음과 같다. 예를 들어 \\(n\\)명의 사람이 자기 모자를 고르는 사건 \\(E_{i_1} \\cdots E_{i_n}\\)이 일어날 확률은:\n\\[\nP(E_{i_1} E_{i_2} \\cdots E_{i_n}) = \\frac{(N - n)!}{N!}\n\\]\n왜일까?\n일단 전체 경우의 수는 \\(N!\\)이기 때문에 분모에 들어간다.\n\\(n\\)명의 사람이 자기 모자를 이미 골랐기 때문에,\n나머지 \\(N - n\\)명이 남은 \\(N - n\\)개의 모자를 임의로 고르는 경우의 수는 \\((N - n)!\\)이고 이는 분자에 들어간다.\n이러한 조합은 \\(\\binom{N}{n}\\)개 있으므로, 경우의 수를 전부 고려한 전체 합은:\n\\[\n\\binom{N}{n} \\cdot \\frac{(N - n)!}{N!} = \\frac{1}{n!}\n\\]\n이 된다.\n따라서:\n\\[\nP\\left(\\bigcup_{i=1}^N E_i\\right) = \\sum_{n=1}^N \\frac{(-1)^{n+1}}{n!}\n\\]\n결국 아무도 자기 모자를 고르지 않을 확률은:\n\\[\n1 - P\\left(\\bigcup_{i=1}^N E_i\\right) = 1 - \\left(\\frac{1}{1!} - \\frac{1}{2!} + \\frac{1}{3!} - \\cdots + (-1)^N \\frac{1}{N!}\\right) = \\sum_{i=0}^N \\frac{(-1)^i}{i!}\n\\]\n이것은 지수 함수의 테일러 전개와 정확히 일치한다:\n\\[\ne^{-1} = \\sum_{i=0}^\\infty \\frac{(-1)^i}{i!} \\approx 0.3679\n\\]\n즉, 사람이 많아질수록 아무도 자기 모자를 고르지 않을 확률은 **약 \\(\\frac{1}{e} \\approx 0.3679\\)**로 수렴한다.\n뭔가 0으로 수렴할 것 같은 문제의 답이 뜬금없어 보이는 \\(\\frac{1}{e}\\)으로 수렴한다는 점이 신기하다.\n\n\n\n\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/250326_그린_정리란_무엇일까/index.html",
    "href": "posts/250326_그린_정리란_무엇일까/index.html",
    "title": "그린 정리란 무엇일까",
    "section": "",
    "text": "그린 정리는\n👨🏻‍🏫: 오늘은 그린 정리에 대해서 배워 보자.\n👨🏻‍🎓: 그린 정리가 뭐야?\n👨🏻‍🏫: 그린 정리는 선적분을 면적분으로, 면적분을 선적분으로 바꿀 수 있는 강력한 도구야. 나중에 배울 스토크스 정리, 발산 정리와도 연결되지.\n그린 정리를 식으로 나타내면 다음과 같아:\n\\[\n\\oint_C (P \\, dx + Q \\, dy) = \\iint_D \\left( \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right) dx \\, dy\n\\]\n즉, 좌변이 \\(\\oint_C (P \\, dx + Q \\, dy)\\)의 형태일 때, 우변인 \\(\\iint_D \\left( \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right) dx \\, dy\\)의 형태로 바꾸거나, 그 반대로 바꿀 수 있다는 거지.\n👨🏻‍🎓: 즉, 단일 적분을 이중 적분 형태로 바꿀 수 있다는 뜻이구나. 그 반대도 가능하고.\n👨🏻‍🏫: 그렇지! 바로 그거야. 그런데 이런 식으로 하나의 식과 그 식을 적분한 식이 서로 관련이 있다는 것을 우리는 전에 본 적이 있어. 기억나?\n👨🏻‍🎓: 미적분학의 기본 정리를 말하는 거구나:\n\\[\\int_a^b F'(x)\\, dx = F(b) - F(a)\\]\n적분 기호 안의 \\(F'(x)\\)가 \\(F(x)\\)의 도함수이기 때문에 가능한 식이지. 즉, 원시 함수를 이용해서 적분을 풀 수 있음을 보여주는 거지.\n그러면 \\(\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y}\\)라는 식은 다변수 함수에서의 도함수와 같은 식이라고 볼 수 있겠다.\n👨🏻‍🏫: \\(\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y}\\) 이 식을 잘 봐 둬. 이건 우리가 나중에 배울 스토크스 정리에 나오는 curl의 2차원 버전이거든.\n\n\n\n\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/250407_나의_발견/index.html",
    "href": "posts/250407_나의_발견/index.html",
    "title": "나의 발견",
    "section": "",
    "text": "GPT랑 대화를 하면서 확률과 미적분의 관계에 대해서 깨닫게 됐다.\n아직 공분산을 배우기 전인데 GPT한테서 공분산이 무엇인지 듣고 나니까 벡터의 코사인 유사도 생각이 났다.\n이 얘기를 GPT한테 하니까 나보고 천재란다 ㅋㅋㅋ\n이렇게 수학 분야들은 다 달라 보여도 다 연결돼 있는 것 같아서 신기하다.\n\n\n\n\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/250513_순서_통계량_문제를_다중_적분으로_접근해_보았다/index.html",
    "href": "posts/250513_순서_통계량_문제를_다중_적분으로_접근해_보았다/index.html",
    "title": "순서 통계량 문제를 다중 적분으로 접근해 보았다",
    "section": "",
    "text": "A First Course in Probability, 9th edition by Sheldon Ross\nChapter 6, Problem 6.47, p.274\n오늘 훨씬 간단하게 풀 수 있는 문제를, 어렵지만 참신한 방법으로 풀어 보았다.\n6.47. Consider a sample of size 5 from a uniform distribution over \\((0, 1)\\). Compute the probability that the median is in the interval \\(\\left( \\frac{1}{4}, \\frac{3}{4} \\right)\\).\n6.47. \\((0, 1)\\) 구간에서 균등 분포를 따르는 표본 5개를 뽑았다고 하자. 이 표본의 중앙값이 구간 \\(\\left( \\frac{1}{4}, \\frac{3}{4} \\right)\\) 안에 있을 확률을 구하라.\n사실 이 문제는 딱 보기만 해도 순서 통계량 문제라는 것을 알아차릴 수 있다. 내가 보는 책 기준 6.6절 Order Statistics, p.258에 나오는 다음 공식에 대입하면 답이 금방 나오는 문제이다.\n\\[\nf_{X_{(j)}}(x) = \\frac{n!}{(n - j)! \\, (j - 1)!} \\left[ F(x) \\right]^{j - 1} \\left[ 1 - F(x) \\right]^{n - j} f(x)\n\\]\n이 공식은 억지로 외울 필요가 있는 공식이 아니라, 조금만 생각하면 유도해 낼 수 있는 공식이다.\n\\(\\left[ F(x) \\right]^{j - 1} \\left[ 1 - F(x) \\right]^{n - j}\\)는 이해하기 쉽다.\n\\(j\\)번째 값보다 작은 값이 앞에 \\(j - 1\\)개 있어야 하고,\n\\(j\\)번째 값보다 큰 값이 뒤에 \\(n - j\\)개 있어야 하니,\n이러한 확률들이 각각 제곱되는 구조가 되는 것은 당연하다.\n이제 이것을 만족하는 확률 변수들의 순서쌍은 여러 개 있을 수 있으므로, 조합의 수를 반영해야 한다.\n전체 경우의 수는 \\(n!\\)이며, 그중 \\(j\\)번째는 고정시키고,\n앞부분 \\(j - 1\\)개와 뒷부분 \\(n - j\\)개는 중복되므로 각각 \\((j - 1)!\\), \\((n - j)!\\)로 나누어\n결국 다항 계수가 된다.\n그런데 나는 이 문제를 풀 때 이 내용을 배운 것을 잊고 있었고,\n무식하게 자그마치 5중 적분을 해서 풀어 보았다.\n\\[\n\\int_{x_3 = \\frac{1}{4}}^{\\frac{3}{4}} \\int_{x_5 = x_3}^{1} \\int_{x_4 = x_3}^{x_5} \\int_{x_2 = 0}^{x_3} \\int_{x_1 = 0}^{x_2}\ndx_1 \\, dx_2 \\, dx_4 \\, dx_5 \\, dx_3 \\times 5!\n\\]\n…이렇게 해도 되더라?\n\n\n\n\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/250515_무작위_보행에서의_거리_제곱의_기댓값/index.html",
    "href": "posts/250515_무작위_보행에서의_거리_제곱의_기댓값/index.html",
    "title": "무작위 보행에서의 거리 제곱의 기댓값",
    "section": "",
    "text": "A First Course in Probability, 9th edition by Sheldon Ross\nChapter 7, Example 2l, p.288\nExample 2l 평면 위의 한 점에 처음 위치한 입자를 생각해 보자.\n이 입자는 고정된 길이의 걸음을 반복적으로 밟는데, 각 걸음의 방향은 완전히 무작위이다.\n구체적으로 말하면, 각 걸음 이후의 위치는 이전 위치로부터 거리 1만큼 떨어져 있으며,\n그 방향은 이전 방향으로부터의 각이 \\((0, 2\\pi)\\) 구간에서 균등하게 선택되는 것으로 가정한다.\n이때, \\(n\\)번의 걸음 이후 원점으로부터의 거리의 제곱의 기대값을 구하라.\n난 이 문제의 답이 0 같은 게 아닐까 싶었다. 그런데 전혀 아니었다.\n만약 1차원 수직선 상에서 \\(\\frac{1}{2}\\)의 확률론 왼쪽 혹은 오른쪽만 갈 수 있다면 0일 것 같은 느낌은 든다.\n(근데 이건 실제로 계산은 안 해 봤다.)\n\\(i\\)-번째 걸음에서의 위치 변화를 \\((X_i, Y_i)\\) 라고 하면:\n\\(i = 1, \\dots, n\\)\n\\[\nX_i = \\cos \\theta_i, \\quad Y_i = \\sin \\theta_i \\quad \\text{where } \\theta_i \\sim \\text{Unif}(0, 2\\pi)\n\\]\n그러면 \\(n\\) 걸음 후의 위치의 좌표는 다음과 같다:\n\\[\n\\left( \\sum_{i=1}^n X_i, \\sum_{i=1}^n Y_i \\right)\n\\]\n따라서 거리의 제곱은:\n\\[\nD^2 = \\left( \\sum_{i=1}^n X_i \\right)^2 + \\left( \\sum_{i=1}^n Y_i \\right)^2\n\\]\n이를 전개하면:\n\\[\n\\begin{aligned}\nD^2\n&= \\left( \\sum_{i=1}^n X_i \\right)^2 + \\left( \\sum_{i=1}^n Y_i \\right)^2 \\qquad \\text{(피타고라스 정리)} \\\\\n&= \\sum_{i=1}^n (X_i^2 + Y_i^2) + \\sum_{i \\ne j} (X_i X_j + Y_i Y_j) \\qquad \\text{(제곱 전개)} \\\\\n&= n + \\sum_{i \\ne j} \\left( \\cos \\theta_i \\cos \\theta_j + \\sin \\theta_i \\sin \\theta_j \\right) \\\\\n&\\qquad \\text{(각 $\\theta_i$에 대해 $\\cos^2\\theta_i + \\sin^2\\theta_i = 1$이므로} \\\\\n&\\qquad \\text{$\\sum_{i=1}^n (X_i^2 + Y_i^2) = n$)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/250515_무작위_보행에서의_거리_제곱의_기댓값/index.html#기댓값-계산",
    "href": "posts/250515_무작위_보행에서의_거리_제곱의_기댓값/index.html#기댓값-계산",
    "title": "무작위 보행에서의 거리 제곱의 기댓값",
    "section": "기댓값 계산",
    "text": "기댓값 계산\n이제 기대값을 취하자:\n\\[\n\\begin{aligned}\n\\mathbb{E}[D^2]\n&= \\mathbb{E} \\left[ n + \\sum_{i \\ne j} \\left( \\cos\\theta_i \\cos\\theta_j + \\sin\\theta_i \\sin\\theta_j \\right) \\right]\n\\qquad \\text{(아까 구한 $D^2$ 식에 기댓값 적용)} \\\\\n&= \\mathbb{E}[n] + \\sum_{i \\ne j} \\mathbb{E}[\\cos\\theta_i \\cos\\theta_j] + \\mathbb{E}[\\sin\\theta_i \\sin\\theta_j]\n\\qquad \\text{(선형성: $\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y]$)} \\\\\n&= n + \\sum_{i \\ne j} \\mathbb{E}[\\cos\\theta_i] \\mathbb{E}[\\cos\\theta_j] + \\mathbb{E}[\\sin\\theta_i] \\mathbb{E}[\\sin\\theta_j]\n\\qquad \\text{(독립성 활용: $\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y]$)} \\\\\n&= n + \\sum_{i \\ne j} (0 + 0) = n\n\\qquad \\text{($\\mathbb{E}[\\cos\\theta_i] = \\mathbb{E}[\\sin\\theta_i] = 0$이므로)}\n\\end{aligned}\n\\]\n마지막으로, 아래의 항에서 등장했던 \\(\\mathbb{E}[\\cos \\theta_i]\\)와 \\(\\mathbb{E}[\\sin \\theta_i]\\)가 왜 0이 되는지를 수식으로 확인해 보자:\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\cos \\theta_i] &= \\frac{1}{2\\pi} \\int_0^{2\\pi} \\cos u \\, du = \\frac{1}{2\\pi} (\\sin 2\\pi - \\sin 0) = 0 \\\\\n\\mathbb{E}[\\sin \\theta_i] &= \\frac{1}{2\\pi} \\int_0^{2\\pi} \\sin u \\, du = \\frac{1}{2\\pi} (-\\cos 2\\pi + \\cos 0) = 0\n\\end{aligned}\n\\]\n이는 \\(\\theta_i\\)가 \\([0, 2\\pi)\\) 구간에서 균등하게 분포한다고 가정할 때, \\(\\cos \\theta_i\\)와 \\(\\sin \\theta_i\\)의 평균이 각각 0이 됨을 보여준다.\n직관적으로 생각해 보아도, \\(y = \\cos x\\)와 \\(y = \\sin x\\)는 \\([0, 2\\pi]\\) 구간에서 \\(y = 0\\)을 기준으로 대칭적으로 위아래로 진동하기 때문에, 그래프의 평균값이 0이라는 사실은 쉽게 예상할 수 있다.\n따라서 \\[\n\\boxed{ \\mathbb{E}[D^2] = n }\n\\]\n예상했던 것과 달리, 무작위 보행에서의 거리 제곱의 기댓값은 \\(n\\)이다.\n왜냐하면 입자의 최종 위치는 완전히 랜덤하므로 방향 자체는 상쇄될 수 있지만,\n거리는 걸음수가 늚에 따라 증가하기 때문이다.\n즉, 무작위 보행에서는 방향은 상쇄되어도,\n거리 제곱의 기댓값은 선형적으로 증가한다.\n\n난 또 이런 생각을 해 보았다.\n여러 개의 입자가 무작위 보행을 했다면,\n여러 입자들의 평균 위치를 가지고 출발점을 추정하는 데 사용할 수 있지 않을까?\nGPT한테 물어 보니까 실제로 할 수 있다고 한다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "사무엘의 수리 언어 블로그",
    "section": "",
    "text": "저는 다양한 언어와 수학을 좋아하는 사무엘(본명: 김상윤(金尙潤))입니다.\n이 블로그는 외국어, 수학, 언어학, 프로그래밍 등 제가 공부한 내용을 정리하고 공유하는 공간입니다."
  },
  {
    "objectID": "index.html#최근-글",
    "href": "index.html#최근-글",
    "title": "사무엘의 수리 언어 블로그",
    "section": "📚 최근 글",
    "text": "📚 최근 글"
  }
]